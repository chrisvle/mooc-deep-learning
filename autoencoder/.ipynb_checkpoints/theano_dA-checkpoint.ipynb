{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "from logistic_sgd import load_data\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dA(object):\n",
    "    \"\"\"Denoising Auto-Encoder class (dA)\n",
    "\n",
    "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
    "    version of it by projecting it first in a latent space and reprojecting\n",
    "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
    "    for more details. If x is the input then equation (1) computes a partially\n",
    "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
    "    computes the projection of the input into the latent space. Equation (3)\n",
    "    computes the reconstruction of the input, while equation (4) computes the\n",
    "    reconstruction error.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
    "\n",
    "        y = s(W \\tilde{x} + b)                                           (2)\n",
    "\n",
    "        x = s(W' y  + b')                                                (3)\n",
    "\n",
    "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        bhid=None,\n",
    "        bvis=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dA class by specifying the number of visible units (the\n",
    "        dimension d of the input ), the number of hidden units ( the dimension\n",
    "        d' of the latent or hidden space ) and the corruption level. The\n",
    "        constructor also receives symbolic variables for the input, weights and\n",
    "        bias. Such a symbolic variables are useful when, for example the input\n",
    "        is the result of some computations, or when weights are shared between\n",
    "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
    "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
    "        and the weights of the dA are used in the second stage of training\n",
    "        to construct an MLP.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: number random generator used to generate weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                     generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: a symbolic description of the input or None for\n",
    "                      standalone dA\n",
    "\n",
    "        :type n_visible: int\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden:  number of hidden units\n",
    "\n",
    "        :type W: theano.tensor.TensorType\n",
    "        :param W: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "\n",
    "        :type bhid: theano.tensor.TensorType\n",
    "        :param bhid: Theano variable pointing to a set of biases values (for\n",
    "                     hidden units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "        :type bvis: theano.tensor.TensorType\n",
    "        :param bvis: Theano variable pointing to a set of biases values (for\n",
    "                     visible units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformely sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if not bvis:\n",
    "            bvis = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if not bhid:\n",
    "            bhid = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        self.W = W\n",
    "        # b corresponds to the bias of the hidden\n",
    "        self.b = bhid\n",
    "        # b_prime corresponds to the bias of the visible\n",
    "        self.b_prime = bvis\n",
    "        # tied weights, therefore W_prime is W transpose\n",
    "        self.W_prime = self.W.T\n",
    "        self.theano_rng = theano_rng\n",
    "        # if no input is given, generate a variable representing the input\n",
    "        if input is None:\n",
    "            # we use a matrix because we expect a minibatch of several\n",
    "            # examples, each example being a row\n",
    "            self.x = T.dmatrix(name='input')\n",
    "        else:\n",
    "            self.x = input\n",
    "\n",
    "        self.params = [self.W, self.b, self.b_prime]\n",
    "\n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
    "        same and zero-out randomly selected subset of size ``coruption_level``\n",
    "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
    "               random numbers that it should produce\n",
    "               second argument is the number of trials\n",
    "               third argument is the probability of success of any trial\n",
    "\n",
    "                this will produce an array of 0s and 1s where 1 has a\n",
    "                probability of 1 - ``corruption_level`` and 0 with\n",
    "                ``corruption_level``\n",
    "\n",
    "                The binomial function return int64 data type by\n",
    "                default.  int64 multiplicated by the input\n",
    "                type(floatX) always return float64.  To keep all data\n",
    "                in floatX when floatX is float32, we set the dtype of\n",
    "                the binomial to floatX. As in our case the value of\n",
    "                the binomial is always 0 or 1, this don't change the\n",
    "                result. This is needed to allow the gpu to work\n",
    "                correctly as it only support float32 for now.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n",
    "\n",
    "    def get_hidden_values(self, input):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "\n",
    "    def get_reconstructed_input(self, hidden):\n",
    "        \"\"\"Computes the reconstructed input given the values of the\n",
    "        hidden layer\n",
    "\n",
    "        \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "\n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng\n",
    "        step of the dA \"\"\"\n",
    "\n",
    "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        # note : we sum over the size of a datapoint; if we are using\n",
    "        #        minibatches, L will be a vector, with one entry per\n",
    "        #        example in minibatch\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        cost = T.mean(L)\n",
    "\n",
    "        # compute the gradients of the cost of the `dA` with respect\n",
    "        # to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        return (cost, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper to get MNIST\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "def load_data(dataset):\n",
    "    ''' Loads the dataset\n",
    " \n",
    "    :type dataset: string\n",
    "    :param dataset: the path to the dataset (here MNIST)\n",
    "    '''\n",
    " \n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    " \n",
    "    # Download the MNIST dataset if it is not present\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(os.path.split(__file__)[0], \"..\", \"data\", dataset)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    " \n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        import urllib\n",
    "        origin = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        print 'Downloading data from %s' % origin\n",
    "        urllib.urlretrieve(origin, dataset)\n",
    " \n",
    "    print '... loading data'\n",
    " \n",
    "    # Load the dataset\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    #train_set, valid_set, test_set format: tuple(input, target)\n",
    "    #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "    #which row's correspond to an example. target is a\n",
    "    #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "    #the number of rows in the input. It should give the target\n",
    "    #target to the example with the same index in the input.\n",
    " \n",
    "#     return (train_set, valid_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_A(learning_rate=0.1, training_epochs=1,\n",
    "            dataset='mnist.pkl.gz',\n",
    "            batch_size=20, output_folder='dA_plots'):\n",
    "\n",
    "    \"\"\"\n",
    "    This demo is tested on MNIST\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used for training the DeNosing\n",
    "                          AutoEncoder\n",
    "\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: number of epochs used for training\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path to the picked dataset\n",
    "\n",
    "    \"\"\"\n",
    "    datasets = load_data(dataset)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    os.chdir(output_folder)\n",
    "\n",
    "    ####################################\n",
    "    # BUILDING THE MODEL NO CORRUPTION #\n",
    "    ####################################\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    da = dA(\n",
    "        numpy_rng=rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "        n_visible=28 * 28,\n",
    "        n_hidden=500\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=0.,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_da = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "\n",
    "#     print >> sys.stderr, ('The no corruption code for file ' +\n",
    "#                           os.path.split(__file__)[1] +\n",
    "#                           ' ran for %.2fm' % ((training_time) / 60.))\n",
    "    image = Image.fromarray(\n",
    "        tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
    "                           img_shape=(28, 28), tile_shape=(10, 10),\n",
    "                           tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_dA(learning_rate=0.1, training_epochs=1,\n",
    "            dataset='mnist.pkl.gz',\n",
    "            batch_size=20, output_folder='dA_plots'):\n",
    "    \n",
    "    datasets = load_data(dataset)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    # start-snippet-2\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.matrix('x')  # the data is presented as rasterized images\n",
    "    # end-snippet-2\n",
    "\n",
    "    if not os.path.isdir(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    os.chdir(output_folder)\n",
    "    \n",
    "    #####################################\n",
    "    # BUILDING THE MODEL CORRUPTION 30% #\n",
    "    #####################################\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    da = dA(\n",
    "        numpy_rng=rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "        n_visible=28 * 28,\n",
    "        n_hidden=500\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=0.3,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_da = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "\n",
    "#     print >> sys.stderr, ('The 30% corruption code for file ' +\n",
    "#                           os.path.split(__file__)[1] +\n",
    "#                           ' ran for %.2fm' % (training_time / 60.))\n",
    "\n",
    "    image = Image.fromarray(tile_raster_images(\n",
    "        X=da.W.get_value(borrow=True).T,\n",
    "        img_shape=(28, 28), tile_shape=(10, 10),\n",
    "        tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_30.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/../data/mnist.pkl.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d7e92a107010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-3ccb78b24b3c>\u001b[0m in \u001b[0;36mtest_A\u001b[0;34m(learning_rate, training_epochs, dataset, batch_size, output_folder)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-ba13f0f4e7d9>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     34\u001b[0m         )\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'Downloading data from %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'... loading data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0murlcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mtfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/../data/mnist.pkl.gz'"
     ]
    }
   ],
   "source": [
    "test_A()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
