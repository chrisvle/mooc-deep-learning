{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import theano as th\n",
    "from theano import tensor as T\n",
    "from numpy import random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(object):\n",
    "    \n",
    "    def __init__(self, X, hidden_size, activation_function,\n",
    "                 output_function, theano_rng = None, denoising=False):\n",
    "        # X is the data, an m x n numpy matrix\n",
    "        # where rows correspond to datapoints\n",
    "        # and columns correspond to features.\n",
    "        assert type(X) is np.ndarray\n",
    "        assert len(X.shape) == 2\n",
    "        \n",
    "        self.X=X\n",
    "        self.X=th.shared(name='X', value=np.asarray(self.X, \n",
    "                         dtype=th.config.floatX),borrow=True)\n",
    "        \n",
    "        #The config.floatX and borrow=True stuff is to get this to run\n",
    "        #fast on the gpu. I recommend just doing this without thinking about\n",
    "        #it until you understand the code as a whole, then learning more\n",
    "        #about gpus and theano.\n",
    "        \n",
    "        self.n = X.shape[1]\n",
    "        self.m = X.shape[0]\n",
    "        \n",
    "        #Hidden_size is the number of neurons in the hidden layer, an int.\n",
    "        \n",
    "        assert type(hidden_size) is int\n",
    "        assert hidden_size > 0\n",
    "        \n",
    "        self.hidden_size=hidden_size\n",
    "        initial_W = np.asarray(rng.uniform(\n",
    "                 low=-4 * np.sqrt(6. / (self.hidden_size + self.n)),\n",
    "                 high=4 * np.sqrt(6. / (self.hidden_size + self.n)),\n",
    "                 size=(self.n, self.hidden_size)), dtype=th.config.floatX)\n",
    "        \n",
    "        # weights\n",
    "        self.W = th.shared(value=initial_W, name='W', borrow=True)\n",
    "        \n",
    "        # bias 1\n",
    "        self.b1 = th.shared(name='b1', value=np.zeros(shape=(self.hidden_size,),\n",
    "                            dtype=th.config.floatX),borrow=True)\n",
    "        \n",
    "        # bias 2\n",
    "        self.b2 = th.shared(name='b2', value=np.zeros(shape=(self.n,),\n",
    "                            dtype=th.config.floatX),borrow=True)\n",
    "        self.activation_function = activation_function\n",
    "        self.output_function = output_function\n",
    "        self.denoising = denoising\n",
    "        \n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "                     \n",
    "    def train(self, n_epochs=100, mini_batch_size=1, learning_rate=0.1, corruption_level = 0):\n",
    "        index = T.lscalar()\n",
    "        x = T.matrix('x')\n",
    "        params = [self.W, self.b1, self.b2]\n",
    "        hidden = self.activation_function(T.dot(x, self.W)+self.b1)\n",
    "        output = T.dot(hidden,T.transpose(self.W))+self.b2\n",
    "        output = self.output_function(output)\n",
    "         \n",
    "        # Use cross-entropy loss.\n",
    "        L = -T.sum(x*T.log(output) + (1-x)*T.log(1-output), axis=1)\n",
    "        cost = L.mean()       \n",
    "        updates = []\n",
    "         \n",
    "        # Return gradient with respect to W, b1, b2.\n",
    "        gparams = T.grad(cost,params)\n",
    "         \n",
    "        # Create a list of 2 tuples for updates.\n",
    "        for param, gparam in zip(params, gparams):\n",
    "            updates.append((param, param-learning_rate*gparam))\n",
    "         \n",
    "        # Train given a mini-batch of the data.\n",
    "        \n",
    "        # If denoising...\n",
    "        if self.denoising:\n",
    "            cost, updates = self.get_cost_updates(corruption_level = corruption_level, learning_rate = learning_rate)\n",
    "            \n",
    "        cost = [cost]  \n",
    "        \n",
    "        train = th.function(inputs=[index], outputs=cost, updates=updates,\n",
    "                            givens={x:self.X[index:index+mini_batch_size,:]})\n",
    "                             \n",
    " \n",
    "        import time\n",
    "        start_time = time.clock()\n",
    "        for epoch in xrange(n_epochs):\n",
    "            print \"Epoch:\",epoch\n",
    "            for row in xrange(0,self.m, mini_batch_size):\n",
    "                train(row)\n",
    "        end_time = time.clock()\n",
    "        print \"Average time per epoch=\", (end_time-start_time)/n_epochs\n",
    "                    \n",
    "    def get_hidden(self,data):\n",
    "        x = T.dmatrix('x')\n",
    "        hidden = self.activation_function(T.dot(x,self.W)+self.b1)\n",
    "        transformed_data = th.function(inputs=[x], outputs=[hidden])\n",
    "        return transformed_data(data)\n",
    "     \n",
    "    def get_weights(self):\n",
    "        return [self.W.get_value(), self.b1.get_value(), self.b2.get_value()]\n",
    "    \n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
    "        same and zero-out randomly selected subset of size ``coruption_level``\n",
    "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
    "               random numbers that it should produce\n",
    "               second argument is the number of trials\n",
    "               third argument is the probability of success of any trial\n",
    "\n",
    "                this will produce an array of 0s and 1s where 1 has a\n",
    "                probability of 1 - ``corruption_level`` and 0 with\n",
    "                ``corruption_level``\n",
    "\n",
    "                The binomial function return int64 data type by\n",
    "                default.  int64 multiplicated by the input\n",
    "                type(floatX) always return float64.  To keep all data\n",
    "                in floatX when floatX is float32, we set the dtype of\n",
    "                the binomial to floatX. As in our case the value of\n",
    "                the binomial is always 0 or 1, this don't change the\n",
    "                result. This is needed to allow the gpu to work\n",
    "                correctly as it only support float32 for now.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n",
    "    \n",
    "    def get_reconstructed_input(self, hidden):\n",
    "        \"\"\"Computes the reconstructed input given the values of the\n",
    "        hidden layer\n",
    "\n",
    "        \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "    \n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng\n",
    "        step of the dA \"\"\"\n",
    "\n",
    "        tilde_x = self.get_corrupted_input(self.X, corruption_level)\n",
    "        y = self.get_hidden(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        # note : we sum over the size of a datapoint; if we are using\n",
    "        #        minibatches, L will be a vector, with one entry per\n",
    "        #        example in minibatch\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        cost = T.mean(L)\n",
    "\n",
    "        # compute the gradients of the cost of the `dA` with respect\n",
    "        # to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        return (cost, updates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper to get MNIST\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "def load_data(dataset):\n",
    "    ''' Loads the dataset\n",
    " \n",
    "    :type dataset: string\n",
    "    :param dataset: the path to the dataset (here MNIST)\n",
    "    '''\n",
    " \n",
    "    #############\n",
    "    # LOAD DATA #\n",
    "    #############\n",
    " \n",
    "    # Download the MNIST dataset if it is not present\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        # Check if dataset is in the data directory.\n",
    "        new_path = os.path.join(os.path.split(__file__)[0], \"..\", \"data\", dataset)\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            dataset = new_path\n",
    " \n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        import urllib\n",
    "        origin = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        print 'Downloading data from %s' % origin\n",
    "        urllib.urlretrieve(origin, dataset)\n",
    " \n",
    "    print '... loading data'\n",
    " \n",
    "    # Load the dataset\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    #train_set, valid_set, test_set format: tuple(input, target)\n",
    "    #input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "    #which row's correspond to an example. target is a\n",
    "    #numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "    #the number of rows in the input. It should give the target\n",
    "    #target to the example with the same index in the input.\n",
    " \n",
    "    return (train_set, valid_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper to plot filters\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_first_k_numbers(X,k):\n",
    "    m=X.shape[0]\n",
    "    k=min(m,k)\n",
    "    j = int(round(k / 10.0))\n",
    "    \n",
    "    fig, ax = plt.subplots(j,10)\n",
    "   \n",
    "    for i in range(k):\n",
    "\n",
    "        w = X[i,:]\n",
    "\n",
    "        \n",
    "        w = w.reshape(28,28)\n",
    "        ax[i/10, i%10].imshow(w,cmap=plt.cm.gist_yarg,\n",
    "                      interpolation='nearest', aspect='equal')\n",
    "        ax[i/10, i%10].axis('off')\n",
    "\n",
    "    \n",
    "    plt.tick_params(\\\n",
    "        axis='x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    plt.tick_params(\\\n",
    "        axis='y',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off', \n",
    "        right='off',    # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AutoEncoder' object has no attribute 'theano_rng'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-dbba5a0ac3a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplot_first_k_numbers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mm_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-dbba5a0ac3a7>\u001b[0m in \u001b[0;36mm_test\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutput_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenoising\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorruption_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplot_first_k_numbers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ac6d34ab5608>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs, mini_batch_size, learning_rate, corruption_level)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# If denoising...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdenoising\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cost_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorruption_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorruption_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ac6d34ab5608>\u001b[0m in \u001b[0;36mget_cost_updates\u001b[0;34m(self, corruption_level, learning_rate)\u001b[0m\n\u001b[1;32m    133\u001b[0m         step of the dA \"\"\"\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mtilde_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_corrupted_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorruption_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtilde_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reconstructed_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-ac6d34ab5608>\u001b[0m in \u001b[0;36mget_corrupted_input\u001b[0;34m(self, input, corruption_level)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \"\"\"\n\u001b[0;32m--> 120\u001b[0;31m         return self.theano_rng.binomial(size=input.shape, n=1,\n\u001b[0m\u001b[1;32m    121\u001b[0m                                         \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcorruption_level\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                                         dtype=theano.config.floatX) * input\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AutoEncoder' object has no attribute 'theano_rng'"
     ]
    }
   ],
   "source": [
    "path=\"/Users/Chris/Github/deeplearnEDU/autoencoder/mnist.pkl.gz\"\n",
    " \n",
    "data = load_data(path)\n",
    "\n",
    "def m_test(data):\n",
    "    X=data[0][0]\n",
    "    activation_function = T.nnet.sigmoid\n",
    "    output_function=activation_function\n",
    "    A = AutoEncoder(X, 500, activation_function, output_function, denoising = True)\n",
    "    A.train(1,20, corruption_level = 0.3)\n",
    "    W=np.transpose(A.get_weights()[0])\n",
    "    plot_first_k_numbers(W, 100)\n",
    " \n",
    "m_test(data) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
